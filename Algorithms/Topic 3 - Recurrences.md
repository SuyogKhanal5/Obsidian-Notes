
- ### Merge Sort
	- ###### Time Complexity
		- Best case: $O(n \log n)$
		- Average case: $O(n \log n)$
		- Worst case: $O(n \log n)$
	- ###### Space Complexity
		- $O(n)$ (requires additional array)
	- ###### Recurrence
		- $T(n) = 2T(\frac{n}{2})+\frac{n}{2}$

- ### Expansion
	- Find the time complexity of the following recurrence: $T(n) = 2T(\frac{n}{2})+n$
	- Common in divide and conquer algorithms
	- Each problem is divided into 2 subproblems of size $\frac{n}{2}$
	- Additional work per level is linear: $\Theta(n)$
	- ###### Solve using expansion
		- $𝑇(𝑛) = 2 \cdot 𝑇(\frac{𝑛}{2}) + \Theta(n) = 2^{2}\cdot T(\frac{n}{2^2})+n+n$
		- $T(n)=2^{2}\cdot T(\frac{n}{2^2})+n+n$
		- $T(n)=2^3\cdot T(\frac{n}{2^3})+n+n+n$
		- $T(n)=2^{k} \cdot T(\frac{n}{2^{k}})+k \cdot n$
		- Stop when $\frac{n}{2^k}=1$, i.e $k=\log_2 n$
		- Substituting $k=\log_2 n$ gives us $T(n)=n+\log_2 n=O(n\log n)$

- ### Multiplication of Numbers
	- We want to multiply two large numbers $x$ and $y$, where each number is $n$ digits long. Traditionally this takes $O(n^2)$ time
	- We can split each number into two halves: 
		- $x=x_{1}\cdot 10^{\frac{n}{2}}+x_0$
		- $y=y_{1}\cdot 10^{\frac{n}{2}}+y_0$
	- The product $z=xy$ can be computed as follows:
		- $z=(x_{1}\cdot 10^{\frac{n}{2}}+x_0)(y_{1}\cdot 10^{\frac{n}{2}}+y_0)$
	- Expanding this gives us $z=x_{1}y_{1}\cdot 10^{n}+(x_{1}y_{0}+x_{0}y_{1}) \cdot 10^{\frac{n}{2}}+x_0y_0$
	- ###### Karatsuba's Algorithm
		- Reduces number of multiplications from 4 to 3
		- $z_{0}=x_{0}y_{0}$ and $z_{2}=x_{1}y_{1}$
		- We can directly compute $z_1=(x_1+x_0)\cdot (y_1+y_{0})-z_{0}-z_{2}$
		- $z=z_{2}\cdot 10^{\frac{n}{2}}+z_{1}\cdot 10^{\frac{n}{2}}+z_{0}$
	- ###### Recurrence
		- $T(n)=3T(\frac{n}{2})+O(n)$
		- By solving this we get $T(n)=\Theta(n^{\log_{2}3})=O(n^{1.58})$
		- This is asymptotically faster than the $O(n^2)$ method

- ### Matrix Multiplication
	- Divide and conquer also works on matrices
	- We are interested in multiplying two square matrices $A$ and $B$ to compute product matrix $C$, where each entry of $C$ is given by $c_{ij}=\sum_{k=1}^{n}a_{ik} \cdot b_{jk}$
	- A naïve algorithm computes this in $O(n^3)$ time
	- We divide each matrix into four $\frac{n}{2} \times \frac{n}{2}$ submatrices
		- $A=\begin{bmatrix}A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}$ $B=\begin{bmatrix}B_{11} & B_{12} \\ B_{21} & B_{22} \end{bmatrix}$
	- The product matrix $C$ is computed using the following equations:
		- $C=\begin{bmatrix}C_{11} & C_{12} \\ C_{21} & C_{22} \end{bmatrix}=\begin{bmatrix}A_{11} \times B_{11} + A_{12} \times B_{21} & A_{11} \times B_{12} +A_{12} \times B_{22} \\ A_{21} \times B_{11} + A_{22} \times B_{21} & A_{21} \times B_{12} +A_{22} \times B_{22} \end{bmatrix}$
	- ###### Time Complexity of Naive Approach
		- Recurrence for the running time 
			- $T(n)=8T(\frac{n}{2}+O(n^2))$
		- By solving this recurrence we obtain:
			- $T(n)=O(n^3)$
		- This divide and conquer algorithm achieves the same asymptotic time complexity as the naïve approach
	- ###### Strassen's Algorithm
		- $M_{1}=(A_{11}+A_{22})\times (B_{11}+B_{22})$
		- $M_{2}=(A_{21}+A_{22})\times B_{11}$
		- $M_{3}=A_{11}\times(B_{12}-B_{22})$
		- $M_{4}=A_{22}\times(B_{21}-B_{11})$
		- $M_{5}=(A_{11}+A_{12})\times B_{22}$
		- $M_{6}=(A_{21}-A_{11})\times(B_{11}+B_{12})$
		- $M_7=(A_{12}-A_{22})\times (B_{21}+B_{22})$
		- $C=\begin{bmatrix}C_{11} & C_{12} \\ C_{21} & C_{22} \end{bmatrix}=\begin{bmatrix}M_{1}+M_{4}-M_{5}+M_{7} & M_{3}+M_{5} \\ M_{2}+M_{4} & M_{1}-M_{2}+M_{3}+M_{6} \end{bmatrix}$
		- Note that there are only 7 multiplications of smaller matrices
	- ###### Time Complexity of Strassen's Algorithm
		- Recurrence for the running time:
			- $T(n)=7T(\frac{n}{2})+O(n^2)$
		- By solving for the recurrence we obtain:
			- $T(n)=\Theta(n \log_{2}7)=O(n^{2.81})$

- ### Recursive Tree Method
	- Visualize the work done at each level with a recursion tree ![[Drawing 2024-09-22 17.55.38.excalidraw|5000]]

- ### Master Theorem
	- Let $a \ge 1$ and $b > 1$ be constants. Let $f(n)$ be a function, and let $T(n)$ be defined on the non-negative integers by the recurrence $T(n)=aT(\frac{n}{b})+f(n)$
		- If $f(n)=O(n^{\log_{b} a - \epsilon})$ for some constant $\epsilon > 0$ then $T(n)=\Theta(n^{\log_{b} a})$
		- If $f(n)= \Theta(n^{\log_{b} a})$ then $T(n)=\Theta(n^{\log_{b} a} \log n)$
		- If $f(n)=\Omega(\Theta(n^{\log_{b} a + \epsilon}))$ for some constant $\epsilon > 0$ and if $af\left(\frac{n}{b}\right)\le cf(n)$ for some constant $c < 1$ and all sufficiently large $n$, then $T(n)=\Theta(f(n))$
	- Each corresponds to the following:
		- The weight increases geometrically from node to leaves
		- The weight is roughly equal in each layer
		- The weight decreases geometrically from node to leaves

- ### Change of Variables
	- Solve the recurrence $T(n)=2T(\sqrt{n}+\log n)$
	- Use substitution: Let $n=2^m$
	- Now the recurrence becomes $S(m)=2S(\frac{m}{2})+m$
	- This is now a standard recurrence
	- $S(m)=O(m \log m) \Rightarrow T(n) = O(\log n \log \log n)$