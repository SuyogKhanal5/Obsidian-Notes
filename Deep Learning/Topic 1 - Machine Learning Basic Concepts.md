
- ### Artificial Intelligence vs Machine Learning vs Deep Learning
	- **Artificial Intelligence**: Programming systems to perform tasks which usually require human intelligence
		- Examples of systems that are AI but not ML
			- AI chess player using heuristic search algorithms like BFS and DFS
			- AI path finder using heuristic search
	- **Machine Learning**: Training algorithms to solve tasks by pattern recognition instead of specifically programming them how to solve the task
		- Examples of algorithms that are ML but not DL
			- Decision Trees
			- Bayesian Networks
			- K Nearest Neighbors
	- **Deep Learning**: Training algorithms using deep neural networks w/ multiple layers![[Pasted image 20240905155336.png]]

- ### Components of Machine Learning
	- ###### Data
		- $(x,y)$ pair for supervised learning
		- Only $x$ for unsupervised learning
		- $(x_1$, $\dots$ $x_T)$ for sequential data
	- ###### Model
		- Polynomial function, Logistic regression, Neural Networks, etc.
	- ###### Objective
		- Mean Squared Error (MSE), Maximum Likelihood, Posterior Inference, Maximum-A-Posteriori
	- ###### Learning Algorithm
		- Learning Algorithms: Closed-form solution, Gradient descent (backpropagation), MCMC, Variational Inference, etc.
		- Regularization Algorithms: L1/L2 Normalization, Weight decay, dropout, batch normalization, etc.

- ### Types of ML Problems
	- ###### Supervised Learning
		- When an observation has the input-output $D=(x_n,y_n)$ with $y$ as the label of $x$
	- ###### Unsupervised Learning
		- The dataset consists of only observations (inputs) $D=\{x_n\}$
	- ###### Reinforcement Learning
		- Sequential decision making with delayed reward
	- ###### Self-Supervised Learning
		- e.g. Learn to predict future
	- ###### Semi-Supervised Learning
		- Only a part of the data has labels

- ### Supervised Learning
	- ###### Classification
		- Output $y$ is categorical (nominal)
		- Examples
			- Multiclass Classification: Classify object types, digit classes $(0-9)$
			- Binary Classification: Spam email or not
	- ###### Regression
		- Output $y \in \mathbb{R}$
		- Examples
			- Predict stock price
			- Energy consumption
	- ###### Parametric Supervised Learning
		- Typically uses function approximation
		- $y$ is response variable
		- $f()$ is the model
		- $\theta$ is the model parameter and *hyperparameter*
		- $x$ is the features, attributes, and covariates
		- The equation for our function is $y=f_{\theta}(x)$
	- ###### Polynomial Curve Fitting Example
		- $M$ is the polynomial degree of our model
		- $y(x,\vec{w})=w_0+w_1x+w_2x^2+\dots+w_Mx^M=\sum_{j=0}^{M} w_jx^j$
		- $w_0+w_1x+w_2x^2+\dots+w_Mx^M$ is our model while $\sum_{j=0}^{M} w_jx^j$ is our parameters![[Pasted image 20240905160645.png]]
	- ###### Objective Function
		- $E(\vec{w})=\frac{1}{2}\sum_{n=1}^{N} (y(x_{n},\vec{w})-t_{n})^2$![[Pasted image 20240905161448.png]]
	- ###### How to get optimal $\vec{w}$?
		- Learning Algorithm
			- Learning algorithms: Closed-form solution, Gradient Descent (backpropagation), MCMC, Variational Inference, etc.
			- Regularization algorithms: L1/L2 norm, weight decay, dropout, batch normalization, etc.
	- ###### Gradient Descent
		- $y(x,\vec{w})=w_0+w_1x+w_2x^2+\dots+w_Mx^M=\sum_{j=0}^{M} w_jx^j$
		- $\vec{w}=\vec{w}-p\frac{\partial E(\vec{w})}{\partial \vec{w}}$
	- ###### Closed-Form Solution
		- $E(\vec{w})=\frac{1}{2}||X\vec{w}-\vec{t}||^2_2$
		- $E(\vec{w})=\frac{1}{2}(\vec{w}^TX^TX\vec{w}-2(\vec{t}^TX)\vec{w}+\vec{t}^Tt)$
	- ###### Generalization
		- The objective function should be designed to optimize generalization performance
			- Generalization accuracy is the accuracy on the TEST set (not training)
			- Memorization of the training set can provide perfect training error but very poor test performance

- ### Overfitting
	- A good generalization will give low test error which is similar to training error
	- If training error << test error, we say the model is **overfitted**
	- ###### Why does overfitting happen?
		- Because the model is too complex
		- To obtain low training error the model should have high complexity, capacity, and flexibility
		- In general, the more parameters, the more complex the model is. This allows it to capture more complex phenomena but this can lead to overfitting
	- ###### How can we prevent overfitting?
		- Limit the model complexity, we call this **regularization**
		- Model complexity is a function of learnable model parameters and the value range the parameters can take
		- We can make it less complex by limiting the number of parameters, limiting the value range parameters can take, and other techniques like dropout![[Pasted image 20240905165934.png]]
	- ###### Regularization
		- Prevent parameter value from becoming too large
			- $\tilde{E}(\vec{w})=\frac{1}{2}\sum_{n=1}^{N} (y(x_{n},\vec{w})-t_{n})^{2}+ \frac{\lambda}{2}||\vec{w}||^2$
		- $\lambda$ is regularization strength parameter $\lambda \ge 0$
			- This is a hyperparameter which we manually set before training and keep constant during training (*do not learn it*)![[Pasted image 20240905170820.png]]
	- ###### Data Size
		- Proper model complexity depends on training data
			- A model can overfit on small data but may not if we provide more data
		- So often another way to avoid overfitting is to provide more data
			- But often getting more data is expensive
	- ###### How do we know when overfitting happens?
		- Compare the training error and validation error
		- In iterative learning algorithms like gradient descent we do this as the learning proceeds or just use the final errors
		- In non-iterative learning algorithms (e.g. closed form solutions) we can only compare final errors![[Pasted image 20240910161059.png]]

- ### Underfitting
	- A model is underfitting if there is still room to improve the generalization error by using a more complex model or training longer![[Pasted image 20240910161318.png]]![[Pasted image 20240910161412.png]]

- ### Model Search and Hyperparameter Search
	- When we have a variety of models with different complexity
	- Finding a model (often specified by hyperparameters) that provides the best generalization performance
	- Here, a model means any aspect of the learning model (besides the learning parameters) affecting the performance
		- How many hidden units, layers, etc.
		- Types of non-linear activation (sigmoid, tanh, ReLU, etc.)
		- Learning rate of stochastic gradient descent
		- The value of regularization (weight decay) parameter $\lambda$

- ### K-Fold Cross Validation
	- We may overfit to a specific validation set
	- We can do $k$-Fold cross validation
		- Split the non-test data set into $k$-fold, and use one for validation
		- Train a model by using each fold as a validation fold (other $k$-1 folds used as the training set)
		- When $k=N$ (the number of observations), the $k$-fold cross validation is the Leave-One-Out Cross Validation (LOOCV), which is not good for large datasets![[Pasted image 20240910163023.png]]

- ### Unsupervised Learning
	- ###### Types of Problems
		- Knowledge Discovery
			- Finding interesting structure (e.g., clustering structure)![[Pasted image 20240910163843.png]]
		- Discovering Latent Factors / Representation Learning
			- Dimensionality reduction (for visualization or downstream tasks)![[Pasted image 20240910163646.png]]
		- Density Estimation
			- Learning $P(x;\theta)$, Generative Modeling; Anomaly Detection![[Pasted image 20240910163914.png]]
		- Discovering Graph Structure![[Pasted image 20240910163928.png]]
		- Matrix Completion![[Pasted image 20240910163938.png]]
	- Often hard to evaluate
	- ###### K-Means Clustering
			- Given $N$ data points, $x_1,x_{2,\dots}x_n$ finding $K$ clusters
			- Assign each data point $x_i$ into a cluster $k \in \{1,\dots K\}$
			- $K$ is hyperparameter![[Pasted image 20240910165754.png]]
			- Steps
				- Initialization: $k$ initial means are randomly generated within the data domain![[Pasted image 20240910165943.png]]
				- Assignment: $k$ clusters are created by associating every observation with the nearest mean. The partitions here represent the Voronoi diagram generated by the means![[Pasted image 20240910170103.png]]
				- Center Adjustment: The centroid of each of the clusters becomes the new mean
					- In the Gaussian Mixture version we also compute the covariance matrix as k means doesnt work as well in the 4th dimension
				- Assignment and Adjustment are repeated until convergence

- ### Parametric vs Nonparametric Models
	- **Parametric**: The capacity of a learning model $p(y|x;D)$ or $p(x;D)$ depends on a fixed number of parameters $\theta$
		- Fast, memory efficient, but less flexible
			- Strong assumptions on the nature of the data distribution
		- e.g. Polynomial functions or Gaussian distributions
	- **Nonparametric**: The capacity of the model grows with the amount of data not with the number of parameters
		- Nonparametric $\ne$ *no parameters*
		- Often need to store training data at inference time
		- Flexible but slow and memory inefficient for large datasets
		- e.g. K-Nearest Neighbors, Decision Tree, Gaussian Processes

- ### K-Nearest Neighbors
	- Make prediction based on the consensus of $k$ closest neighbors by Euclidean distance
	- Memory based or instance based learning![[Pasted image 20240912163611.png]]
	- ###### Curse of Dimensionality
		- KNN breaks down in high dimensional spaces
		- Space volume grows exponentially to the growing input space
			- To work properly, KNN needs to fill the volume space densely enough
			- Neighboring area becomes increasingly sparser![[Pasted image 20240912164524.png]]![[Pasted image 20240912164534.png]]
	- ###### Density Estimation
		- $p(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$
		- Kernel function or Parzen Window ($N$ data points, $D$ is the dimension of each data point, $h$ is the window width)
			- $\begin{equation*} k(\vec{u}) = \begin{cases} 1, |u_{i}| \le \frac{1}{2}, i = 1, \dots, D  \\ 0, \text{otherwise} \end{cases}\end{equation*}$
			- $p(\vec{x})=\frac{1}{N} \sum_{n=1}^{N} \frac{1}{h^D}k(\frac{\vec{x}-\vec{x_{n}}}{h})$
			- Discontinuity problem
		- With a smoother kernel function
			- $p(\vec{x})=\frac{1}{N} \sum_{n=1}^{N} \frac{1}{(2\pi h^2)^{\frac{1}{2}}}\text{exp}\{-\frac{||\vec{x}-\vec{x_{n}}||^2}{2h^2}\}$
			- The density model is obtained by placing a Gaussian over each data point and then adding up the contributions over the whole data set, and then dividing by $N$ such that the density is correctly normalized

- ### No Free Lunch Theorem
	- "All models are wrong but some models are useful" - George Box
	- No universally best model, we should use assumptions (inductive bias) that fit well to the given task or dataset
		- A set of assumptions made for one dataset might not work for another
	- Consequently we must develop many different algorithms

- ### Bias and Inductive Bias
	- ###### Restriction Bias
		- Machine Learning: In the model (hypothesis) space $H$, search for the best model $h* \in H$
		- The range of models $H$ is restriction bias (e.g. The set of all polynomial functions)
	- ###### Preference Bias -> Inductive Bias
		- Among similar models $h_1,h_{2},\dots h_{n}\in H$ (e.g. with identical training error), which one does the algorithm prefer
		- Ex. L2 regularized polynomial regression prefers smaller $\vec{w}$ with smaller $||\vec{w}||_{2}^{2}$
	- A classic example of inductive bias is Occam's razor
		- The simplest consistent hypothesis is the best

- 